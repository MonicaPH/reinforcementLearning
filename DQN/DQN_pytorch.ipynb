{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from lib import plots\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor:\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\" \n",
    "    def process(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state)\n",
    "        state = torch.reshape(state,\n",
    "                              shape=(state.size(2), state.size(0), state.size(1)))\n",
    "        state = transforms.ToPILImage()(state)\n",
    "        processed_state = transforms.functional.to_grayscale(state)\n",
    "        processed_state = transforms.functional.crop(processed_state,\n",
    "                                                     top=34, \n",
    "                                                     left=0, \n",
    "                                                     height=160, \n",
    "                                                     width=160)\n",
    "        processed_state = transforms.functional.resize(processed_state, \n",
    "                                                       size=(84, 84), \n",
    "                                                       interpolation=Image.NEAREST)\n",
    "        processed_state = transforms.ToTensor()(processed_state)\n",
    "        processed_state = torch.squeeze(processed_state)\n",
    "        return processed_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, summaries_dir=None):\n",
    "        super(Estimator, self).__init__()\n",
    "        self.summary_writer = None\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.linear1 = nn.Linear(64*7*7, 512)\n",
    "        self.linear2 = nn.Linear(512, len(VALID_ACTIONS))\n",
    "        \n",
    "        if summaries_dir:\n",
    "            summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format('pytorch'))\n",
    "            if not os.path.exists(summary_dir):\n",
    "                os.makedirs(summary_dir)\n",
    "            self.summary_writer = SummaryWriter()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          s: State input of shape [batch_size, 4, 84, 84]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        s = torch.FloatTensor(s)\n",
    "        s /= 255.0\n",
    "        return self.forward(s)\n",
    "    \n",
    "    def update(self, s, a, y, optimizer):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          s: State input of shape [batch_size, 4, 84, 84]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        self.actions_pl = torch.tensor(a, dtype=torch.int32)\n",
    "        self.y_pl = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        s = torch.FloatTensor(s)\n",
    "        s /= 255.0\n",
    "        \n",
    "        self.preds = self.forward(s)\n",
    "        batch_size = s.shape[0]\n",
    "        gather_indices = torch.arange(batch_size) * self.preds.shape[1] + self.actions_pl\n",
    "        self.action_predictions = torch.gather(self.preds.reshape((-1,)), 0, gather_indices)\n",
    "        \n",
    "        self.losses = (self.action_predictions - self.y_pl)**2\n",
    "        self.loss = self.losses.mean()\n",
    "        self.loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # summaries for tensorboard\n",
    "        if self.summary_writer is not None:\n",
    "            self.summary_writer.add_scalar('Loss', self.loss)\n",
    "            self.summary_writer.add_histogram('losses', self.losses)\n",
    "            self.summary_writer.add_histogram('q_values_hist', self.preds)\n",
    "            self.summary_writer.add_scalar('q_values_max', self.preds.max())\n",
    "        return self.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), \n",
    "                                lr=0.00025, \n",
    "                                alpha=0.99, \n",
    "                                momentum=0.0, \n",
    "                                eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "torch.Size([84, 84])\n",
      "(4, 84, 84)\n",
      "(2, 4, 84, 84)\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([[ 0.0123,  0.0019,  0.0498, -0.0078],\n",
      "        [ 0.0123,  0.0019,  0.0498, -0.0078]], grad_fn=<AddmmBackward>)\n",
      "100.05950164794922\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "sp = StateProcessor()\n",
    "\n",
    "    \n",
    "# Example observation batch\n",
    "observation = env.reset()\n",
    "print(observation.shape)\n",
    "    \n",
    "observation_p = sp.process(observation)\n",
    "print(observation_p.shape)\n",
    "observation = np.stack([observation_p] * 4, axis=0)\n",
    "print(observation.shape)\n",
    "observations = np.array([observation] * 2)\n",
    "print(observations.shape)\n",
    "\n",
    "    # Test Prediction\n",
    "print(type(observations))\n",
    "print(model.predict(observations))\n",
    "\n",
    "    # Test training step\n",
    "y = np.array([10.0, 10.0])\n",
    "a = np.array([1, 3])\n",
    "print(model.update(observations, a, y,optimizer))\n",
    "print('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(model_1, model_2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    model_2.load_state_dict(model_1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values.detach().numpy())\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    optimizer,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sample when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plots.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"pytorch_monitor\")\n",
    "    print(monitor_path)\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        print(\"created monitor dir\")\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    \n",
    "    # TODO: Load and save checkpoints\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(checkpoint_path))\n",
    "        q_estimator.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # TODO: init total_t as global step torch variable\n",
    "    total_t = 0\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(q_estimator, len(VALID_ACTIONS))\n",
    "    \n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(state)\n",
    "    state = np.stack([state] * 4, axis=0)\n",
    "    \n",
    "    for i in range(replay_memory_init_size):\n",
    "        # populate replay memory\n",
    "        action_probs = policy(state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = state_processor.process(next_state)\n",
    "        next_state = np.append(state[1:, :, :], np.expand_dims(next_state, 0), axis=0)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = state_processor.process(state)\n",
    "            state = np.stack([state] * 4, axis=0)\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    print(len(replay_memory))\n",
    "    env = Monitor(env,\n",
    "                directory=monitor_path,\n",
    "                resume=True,\n",
    "                video_callable=lambda count: count % record_video_every == 0)\n",
    "        \n",
    "    for i_episode in range(num_episodes):\n",
    "        # TODO: Save checkpoint or models here\n",
    "        torch.save(q_estimator.state_dict(), checkpoint_path)\n",
    "        # reset env\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(state)\n",
    "        state = np.stack([state]*4, axis=0)\n",
    "        loss = None\n",
    "            \n",
    "        for t in itertools.count():\n",
    "            # get epsilon for time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "                \n",
    "            # TODO: add epsilon summary to q_estimator's summary\n",
    "            q_estimator.summary_writer.add_scalar(scalar_value=epsilon, tag=\"epsilon\", global_step=i_episode)\n",
    "\n",
    "                \n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(q_estimator, target_estimator)\n",
    "                \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "                \n",
    "            # take a step in environment\n",
    "            action_probs = policy(state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = state_processor.process(next_state)\n",
    "            next_state = np.append(state[1:, :, :], np.expand_dims(next_state, 0), axis=0)\n",
    "                \n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            \n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # TODO: Sample a minibatch from the replay memory\n",
    "            sample = random.sample(replay_memory, batch_size)\n",
    "            states_batch, actions_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*sample))\n",
    "            \n",
    "            # TODO: calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch) * discount_factor * np.amax(q_values_next.detach().numpy(), axis=1)\n",
    "            \n",
    "            # TODO: perform SGD\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(states_batch, actions_batch, targets_batch, optimizer)\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "        #TODO: Add summaries to tensorboard\n",
    "        q_estimator.summary_writer.add_scalar(scalar_value=stats.episode_rewards[i_episode], tag=\"episode_reward\",global_step=i_episode)\n",
    "        q_estimator.summary_writer.add_scalar(scalar_value=stats.episode_lengths[i_episode], tag=\"episode_length\",global_step=i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plots.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "    env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Documents/machine-learning/rl/DQN/experiments/Breakout-v0/pytorch_monitor\n",
      "created monitor dir\n",
      "Populating replay memory...\n",
      "50000\n",
      "Step 403 (403) @ Episode 1/50, loss: 0.00080005213385447864\n",
      "Episode Reward: 5.0\n",
      "Step 180 (583) @ Episode 2/50, loss: 0.0008850508020259445\n",
      "Episode Reward: 0.0\n",
      "Step 249 (832) @ Episode 3/50, loss: 0.00059934443561360245\n",
      "Episode Reward: 1.0\n",
      "Step 171 (1003) @ Episode 4/50, loss: 0.00067606312222778894\n",
      "Episode Reward: 0.0\n",
      "Step 276 (1279) @ Episode 5/50, loss: 0.00079801148967817434\n",
      "Episode Reward: 2.0\n",
      "Step 335 (1614) @ Episode 6/50, loss: 0.00065551290754228833\n",
      "Episode Reward: 2.0\n",
      "Step 334 (1948) @ Episode 7/50, loss: 0.00048746418906375766\n",
      "Episode Reward: 3.0\n",
      "Step 176 (2124) @ Episode 8/50, loss: 0.00099464249797165415\n",
      "Episode Reward: 0.0\n",
      "Step 416 (2540) @ Episode 9/50, loss: 0.00083056697621941574\n",
      "Episode Reward: 4.0\n",
      "Step 169 (2709) @ Episode 10/50, loss: 0.0009760853135958314\n",
      "Episode Reward: 0.0\n",
      "Step 280 (2989) @ Episode 11/50, loss: 0.00075621448922902356\n",
      "Episode Reward: 2.0\n",
      "Step 237 (3226) @ Episode 12/50, loss: 0.00068824423942714934\n",
      "Episode Reward: 1.0\n",
      "Step 278 (3504) @ Episode 13/50, loss: 0.00047354522394016385\n",
      "Episode Reward: 2.0\n",
      "Step 173 (3677) @ Episode 14/50, loss: 0.00071229692548513416\n",
      "Episode Reward: 0.0\n",
      "Step 215 (3892) @ Episode 15/50, loss: 0.00104776187799870975\n",
      "Episode Reward: 1.0\n",
      "Step 173 (4065) @ Episode 16/50, loss: 0.0005629041697829962\n",
      "Episode Reward: 0.0\n",
      "Step 237 (4302) @ Episode 17/50, loss: 0.00070843816502019767\n",
      "Episode Reward: 1.0\n",
      "Step 229 (4531) @ Episode 18/50, loss: 0.00094236311269924044\n",
      "Episode Reward: 1.0\n",
      "Step 158 (4689) @ Episode 19/50, loss: 0.00052623689407482746"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-9c6874f1fcc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 batch_size=32):\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-27b49bd83058>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, optimizer, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# TODO: perform SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mstates_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-a53c62b1fd11>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, s, a, y, optimizer)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# env.monitor.close()\n",
    "state_processor = StateProcessor()\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "# Create estimators\n",
    "q_estimator = Estimator(summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator()\n",
    "for t, stats in deep_q_learning(env,\n",
    "                q_estimator=q_estimator,\n",
    "                target_estimator=target_estimator,\n",
    "                state_processor=state_processor,\n",
    "                experiment_dir=experiment_dir,\n",
    "                optimizer=optimizer,\n",
    "                num_episodes=50,\n",
    "                replay_memory_size=500000,\n",
    "                replay_memory_init_size=50000,\n",
    "                update_target_estimator_every=10000, \n",
    "                epsilon_start=1.0,   \n",
    "                epsilon_end=0.1,\n",
    "                epsilon_decay_steps=500000,\n",
    "                discount_factor=0.99,               \n",
    "                batch_size=32):\n",
    "    print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
