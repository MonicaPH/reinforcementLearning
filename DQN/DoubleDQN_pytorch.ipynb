{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from lib import plots\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor:\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\" \n",
    "    def process(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state)\n",
    "        state = torch.reshape(state,\n",
    "                              shape=(state.size(2), state.size(0), state.size(1)))\n",
    "        state = transforms.ToPILImage()(state)\n",
    "        processed_state = transforms.functional.to_grayscale(state)\n",
    "        processed_state = transforms.functional.crop(processed_state,\n",
    "                                                     top=34, \n",
    "                                                     left=0, \n",
    "                                                     height=160, \n",
    "                                                     width=160)\n",
    "        processed_state = transforms.functional.resize(processed_state, \n",
    "                                                       size=(84, 84), \n",
    "                                                       interpolation=Image.NEAREST)\n",
    "        processed_state = transforms.ToTensor()(processed_state)\n",
    "        processed_state = torch.squeeze(processed_state)\n",
    "        return processed_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, summaries_dir=None):\n",
    "        super(Estimator, self).__init__()\n",
    "        self.summary_writer = None\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.linear1 = nn.Linear(64*7*7, 512)\n",
    "        self.linear2 = nn.Linear(512, len(VALID_ACTIONS))\n",
    "        \n",
    "        if summaries_dir:\n",
    "            summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format('pytorch'))\n",
    "            if not os.path.exists(summary_dir):\n",
    "                os.makedirs(summary_dir)\n",
    "            self.summary_writer = SummaryWriter()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          s: State input of shape [batch_size, 4, 84, 84]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        s = torch.FloatTensor(s)\n",
    "        s /= 255.0\n",
    "        return self.forward(s)\n",
    "    \n",
    "    def update(self, s, a, y, optimizer):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          s: State input of shape [batch_size, 4, 84, 84]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        self.actions_pl = torch.tensor(a, dtype=torch.int32)\n",
    "        self.y_pl = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        s = torch.FloatTensor(s)\n",
    "        s /= 255.0\n",
    "        \n",
    "        self.preds = self.forward(s)\n",
    "        batch_size = s.shape[0]\n",
    "        gather_indices = torch.arange(batch_size) * self.preds.shape[1] + self.actions_pl\n",
    "        self.action_predictions = torch.gather(self.preds.reshape((-1,)), 0, gather_indices)\n",
    "        \n",
    "        self.losses = (self.action_predictions - self.y_pl)**2\n",
    "        self.loss = self.losses.mean()\n",
    "        self.loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # summaries for tensorboard\n",
    "        if self.summary_writer is not None:\n",
    "            self.summary_writer.add_scalar('Loss', self.loss)\n",
    "            self.summary_writer.add_histogram('losses', self.losses)\n",
    "            self.summary_writer.add_histogram('q_values_hist', self.preds)\n",
    "            self.summary_writer.add_scalar('q_values_max', self.preds.max())\n",
    "        return self.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Estimator()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), \n",
    "                                lr=0.00025, \n",
    "                                alpha=0.99, \n",
    "                                momentum=0.0, \n",
    "                                eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "torch.Size([84, 84])\n",
      "(4, 84, 84)\n",
      "(2, 4, 84, 84)\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([[-0.0360,  0.0058, -0.0107, -0.0361],\n",
      "        [-0.0360,  0.0058, -0.0107, -0.0361]], grad_fn=<AddmmBackward>)\n",
      "100.30374145507812\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "sp = StateProcessor()\n",
    "\n",
    "    \n",
    "# Example observation batch\n",
    "observation = env.reset()\n",
    "print(observation.shape)\n",
    "    \n",
    "observation_p = sp.process(observation)\n",
    "print(observation_p.shape)\n",
    "observation = np.stack([observation_p] * 4, axis=0)\n",
    "print(observation.shape)\n",
    "observations = np.array([observation] * 2)\n",
    "print(observations.shape)\n",
    "\n",
    "    # Test Prediction\n",
    "print(type(observations))\n",
    "print(model.predict(observations))\n",
    "\n",
    "    # Test training step\n",
    "y = np.array([10.0, 10.0])\n",
    "a = np.array([1, 3])\n",
    "print(model.update(observations, a, y,optimizer))\n",
    "print('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(model_1, model_2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    model_2.load_state_dict(model_1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values.detach().numpy())\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    optimizer,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sample when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plots.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"pytorch_monitor\")\n",
    "    print(monitor_path)\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        print(\"created monitor dir\")\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    \n",
    "    # TODO: Load and save checkpoints\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(checkpoint_path))\n",
    "        q_estimator.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # TODO: init total_t as global step torch variable\n",
    "    total_t = 0\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(q_estimator, len(VALID_ACTIONS))\n",
    "    \n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(state)\n",
    "    state = np.stack([state] * 4, axis=0)\n",
    "    \n",
    "    for i in range(replay_memory_init_size):\n",
    "        # populate replay memory\n",
    "        action_probs = policy(state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = state_processor.process(next_state)\n",
    "        next_state = np.append(state[1:, :, :], np.expand_dims(next_state, 0), axis=0)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = state_processor.process(state)\n",
    "            state = np.stack([state] * 4, axis=0)\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    print(len(replay_memory))\n",
    "    env = Monitor(env,\n",
    "                directory=monitor_path,\n",
    "                resume=True,\n",
    "                video_callable=lambda count: count % record_video_every == 0)\n",
    "        \n",
    "    for i_episode in range(num_episodes):\n",
    "        # TODO: Save checkpoint or models here\n",
    "        torch.save(q_estimator.state_dict(), checkpoint_path)\n",
    "        # reset env\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(state)\n",
    "        state = np.stack([state]*4, axis=0)\n",
    "        loss = None\n",
    "            \n",
    "        for t in itertools.count():\n",
    "            # get epsilon for time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "                \n",
    "            # TODO: add epsilon summary to q_estimator's summary\n",
    "            q_estimator.summary_writer.add_scalar(scalar_value=epsilon, tag=\"epsilon\", global_step=i_episode)\n",
    "\n",
    "                \n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(q_estimator, target_estimator)\n",
    "                \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "                \n",
    "            # take a step in environment\n",
    "            action_probs = policy(state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = state_processor.process(next_state)\n",
    "            next_state = np.append(state[1:, :, :], np.expand_dims(next_state, 0), axis=0)\n",
    "                \n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            \n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # TODO: Sample a minibatch from the replay memory\n",
    "            sample = random.sample(replay_memory, batch_size)\n",
    "            states_batch, actions_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*sample))\n",
    "            \n",
    "            # TODO: calculate q values and targets\n",
    "            q_values_next = q_estimator.predict(next_states_batch)\n",
    "            best_actions = np.argmax(q_values_next.detach().cpu().numpy(), axis=1)\n",
    "            q_values_next_target = target_estimator.predict(next_states_batch)\n",
    "            \n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * \\\n",
    "                discount_factor * q_values_next_target[np.arange(batch_size), best_actions].detach().numpy()\n",
    "            \n",
    "            # TODO: perform SGD\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(states_batch, actions_batch, targets_batch, optimizer)\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "        #TODO: Add summaries to tensorboard\n",
    "        q_estimator.summary_writer.add_scalar(scalar_value=stats.episode_rewards[i_episode], tag=\"episode_reward\",global_step=i_episode)\n",
    "        q_estimator.summary_writer.add_scalar(scalar_value=stats.episode_lengths[i_episode], tag=\"episode_length\",global_step=i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plots.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "    env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.monitor.close()\n",
    "state_processor = StateProcessor()\n",
    "experiment_dir = os.path.abspath(\"./DobleDQN_experiments/{}\".format(env.spec.id))\n",
    "# Create estimators\n",
    "q_estimator = Estimator(summaries_dir=experiment_dir)\n",
    "optimizer = torch.optim.RMSprop(q_estimator.parameters(), \n",
    "                                lr=0.00025, \n",
    "                                alpha=0.99, \n",
    "                                momentum=0.0, \n",
    "                                eps=1e-6)\n",
    "target_estimator = Estimator()\n",
    "for t, stats in deep_q_learning(env,\n",
    "                q_estimator=q_estimator,\n",
    "                target_estimator=target_estimator,\n",
    "                state_processor=state_processor,\n",
    "                experiment_dir=experiment_dir,\n",
    "                optimizer=optimizer,\n",
    "                num_episodes=5,\n",
    "                replay_memory_size=500000,\n",
    "                replay_memory_init_size=500,\n",
    "                update_target_estimator_every=100, \n",
    "                epsilon_start=1.0,   \n",
    "                epsilon_end=0.1,\n",
    "                epsilon_decay_steps=500000,\n",
    "                discount_factor=0.99,               \n",
    "                batch_size=32):\n",
    "    print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
